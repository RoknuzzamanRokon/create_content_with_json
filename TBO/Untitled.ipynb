{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23807fb4-cf3c-4add-94ff-b27953d86444",
   "metadata": {},
   "source": [
    "# Ratehawk data validation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43e247c7-1830-4856-a9a5-4c09ae139d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ab38edc-3d61-4269-8162-95f2d4148a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'D:/ratehawk_data_upload_database/partner_feed_en_v3_full.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930d43b6-7ab5-494a-ae21-2f21bda62b86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2398900-507a-431e-8954-c8220fd8f7aa",
   "metadata": {},
   "source": [
    "### Count total of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cf6d425-24e6-492e-a312-44aa38576f8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (794325839.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 11\u001b[1;36m\u001b[0m\n\u001b[1;33m    total number of rows = 2620000\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file = 'D:/ratehawk_data_upload_database/partner_feed_en_v3_full.csv'\n",
    "total_rows = 0\n",
    "\n",
    "# Read in chunks and count the rows in each chunk\n",
    "for chunk in pd.read_csv(file, chunksize=100000): \n",
    "    total_rows += len(chunk)\n",
    "\n",
    "# print(f\"Total number of rows: {total_rows}\")\n",
    "total number of rows = 2620000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ac1d40-95dc-43bb-adab-88083440d3a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f872fcec-a5b6-46a0-9765-79cd54131286",
   "metadata": {},
   "source": [
    "### Take only headers row only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d93e22c2-f91a-4c13-89c2-811c9d47dec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['address', 'amenity_groups', 'check_in_time', 'check_out_time', 'description_struct', 'id', 'hid', 'images', 'images_ext', 'kind', 'latitude', 'longitude', 'name', 'phone', 'policy_struct', 'postal_code', 'room_groups', 'region', 'star_rating', 'email', 'serp_filters', 'deleted', 'is_closed', 'is_gender_specification_required', 'metapolicy_struct', 'metapolicy_extra_info', 'star_certificate', 'facts', 'payment_methods', 'hotel_chain', 'front_desk_time_start', 'front_desk_time_end', 'keys_pickup']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "file = 'D:/ratehawk_data_upload_database/partner_feed_en_v3_full.csv'\n",
    "\n",
    "\n",
    "df = pd.read_csv(file, nrows = 1)\n",
    "headers = df.columns.tolist()\n",
    "print(headers)\n",
    "\n",
    "# df_header = pd.DataFrame(columns=headers)\n",
    "\n",
    "# print(df_header)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f003fa-4b0f-4263-9446-5d2dac994209",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file, nrows=5)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274040c8-3ed6-4720-a8d0-af855f121b3c",
   "metadata": {},
   "source": [
    "### Show selectted column information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117ff9f2-4c9e-474c-8c33-2d1fa4dee528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file = 'D:/ratehawk_data_upload_database/partner_feed_en_v3_full.csv'\n",
    "\n",
    "desired_columns = [\n",
    "    'address', 'amenity_groups', 'check_in_time', 'check_out_time', \n",
    "    'description_struct', 'id', 'hid', 'images', 'images_ext', \n",
    "    'kind', 'latitude', 'longitude', 'name', 'phone', 'policy_struct'\n",
    "]\n",
    "\n",
    "df = pd.read_csv(file, usecols=desired_columns)\n",
    "\n",
    "print(df.head(2))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac64065-e569-41d6-b5dc-73d693c1816c",
   "metadata": {},
   "source": [
    "### Create column select csv file header row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "79b90ec2-cff3-4c7d-a9e5-3dcb8c8a3340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns added successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, MetaData, Table, Column, Float, Boolean, String\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.sql import text\n",
    "\n",
    "# Database connection setup (replace with your actual database URL)\n",
    "DATABASE_URL = \"mysql+pymysql://root:@localhost/csvdata01_02102024\"\n",
    "engine = create_engine(DATABASE_URL)\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "# Load column headers from the CSV file\n",
    "file = 'D:/ratehawk_data_upload_database/partner_feed_en_v3_full.csv'\n",
    "df = pd.read_csv(file, nrows=1)\n",
    "headers = df.columns.tolist()\n",
    "\n",
    "# Use the existing 'ratehawk' table\n",
    "metadata = MetaData()\n",
    "ratehawk_table = Table('ratehawk', metadata, autoload_with=engine)\n",
    "\n",
    "# Add new columns to the table dynamically\n",
    "for column_name in headers:\n",
    "    if column_name not in ratehawk_table.columns:\n",
    "        # Define the column type\n",
    "        if column_name in ['latitude', 'longitude', 'star_rating']:\n",
    "            new_column = Column(column_name, Float)\n",
    "        elif column_name in ['is_closed', 'is_gender_specification_required', 'deleted']:\n",
    "            new_column = Column(column_name, Boolean)\n",
    "        else:\n",
    "            # For String columns, set a default length (e.g., 255)\n",
    "            new_column = Column(column_name, String(255))\n",
    "\n",
    "        # Prepare and execute the ALTER TABLE statement manually\n",
    "        with engine.connect() as conn:\n",
    "            alter_statement = f\"ALTER TABLE ratehawk ADD COLUMN {column_name} {new_column.type.compile(dialect=engine.dialect)}\"\n",
    "            conn.execute(text(alter_statement))  # Use `text()` to convert the string to an executable object\n",
    "\n",
    "# Commit the changes to the session and close\n",
    "session.commit()\n",
    "session.close()\n",
    "\n",
    "print(\"Columns added successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8b3c3d-c3e9-46b1-87f0-4b36806f598b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, MetaData, Table\n",
    "from sqlalchemy.sql import text\n",
    "import numpy as np\n",
    "\n",
    "# Database connection setup\n",
    "DATABASE_URL = \"mysql+pymysql://root:@localhost/csvdata01_02102024\"\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "# Define chunk size (adjust based on available memory)\n",
    "chunk_size = 10000  # Adjust as needed based on your system's memory\n",
    "\n",
    "# Read the CSV in chunks and process each chunk\n",
    "file = 'D:/ratehawk_data_upload_database/partner_feed_en_v3_full.csv'\n",
    "\n",
    "# Use the existing 'ratehawk' table\n",
    "metadata = MetaData()\n",
    "ratehawk_table = Table('ratehawk', metadata, autoload_with=engine)\n",
    "\n",
    "def handle_nan_values(data):\n",
    "    \"\"\"\n",
    "    Replace NaN values in the dictionary with None.\n",
    "    This ensures compatibility with MySQL.\n",
    "    \"\"\"\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, float) and (pd.isna(value) or np.isnan(value)):\n",
    "            data[key] = None  # Replace NaN with None\n",
    "    return data\n",
    "\n",
    "# Process CSV in chunks\n",
    "for chunk in pd.read_csv(file, chunksize=chunk_size):\n",
    "    # Iterate over each row in the chunk\n",
    "    with engine.connect() as conn:\n",
    "        # Ensure autocommit is enabled for each row\n",
    "        conn.execution_options(autocommit=True)\n",
    "\n",
    "        for _, row in chunk.iterrows():\n",
    "            # Convert the row to a dictionary\n",
    "            data = row.to_dict()\n",
    "\n",
    "            # Ensure that data matches the table columns\n",
    "            filtered_data = {k: v for k, v in data.items() if k in ratehawk_table.columns}\n",
    "\n",
    "            # Handle NaN values before inserting\n",
    "            filtered_data = handle_nan_values(filtered_data)\n",
    "\n",
    "            # Prepare the insert statement\n",
    "            insert_statement = ratehawk_table.insert().values(filtered_data)\n",
    "\n",
    "            # Execute and commit each insert immediately\n",
    "            conn.execute(insert_statement)\n",
    "\n",
    "print(\"CSV data uploaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "44cc67cb-6c3d-42f4-8076-66e5b5068071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV data uploaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, MetaData, Table\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.sql import text\n",
    "import numpy as np\n",
    "\n",
    "# Database connection setup\n",
    "DATABASE_URL = \"mysql+pymysql://root:@localhost/csvdata01_02102024\"\n",
    "engine = create_engine(DATABASE_URL)\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "# Define chunk size (adjust based on available memory)\n",
    "chunk_size = 10000  # Adjust as needed based on your system's memory\n",
    "\n",
    "# Read the CSV in chunks and process each chunk\n",
    "file = 'D:/ratehawk_data_upload_database/partner_feed_en_v3_full.csv'\n",
    "\n",
    "# Use the existing 'ratehawk' table\n",
    "metadata = MetaData()\n",
    "ratehawk_table = Table('ratehawk', metadata, autoload_with=engine)\n",
    "\n",
    "def handle_nan_values(data):\n",
    "    \"\"\"\n",
    "    Replace NaN values in the dictionary with None.\n",
    "    This ensures compatibility with MySQL.\n",
    "    \"\"\"\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, float) and (pd.isna(value) or np.isnan(value)):\n",
    "            data[key] = None  # Replace NaN with None\n",
    "    return data\n",
    "\n",
    "# Process CSV in chunks\n",
    "for chunk in pd.read_csv(file, chunksize=chunk_size):\n",
    "    # Iterate over each row in the chunk\n",
    "    with engine.connect() as conn:\n",
    "        for _, row in chunk.iterrows():\n",
    "            # Convert the row to a dictionary\n",
    "            data = row.to_dict()\n",
    "\n",
    "            # Ensure that data matches the table columns\n",
    "            filtered_data = {k: v for k, v in data.items() if k in ratehawk_table.columns}\n",
    "\n",
    "            # Handle NaN values before inserting\n",
    "            filtered_data = handle_nan_values(filtered_data)\n",
    "\n",
    "            # Prepare the insert statement\n",
    "            insert_statement = ratehawk_table.insert().values(filtered_data)\n",
    "\n",
    "            # Execute the insert statement for each row\n",
    "            conn.execute(insert_statement)\n",
    "\n",
    "# Commit the changes to the session and close\n",
    "session.commit()\n",
    "session.close()\n",
    "\n",
    "print(\"CSV data uploaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea1972b-5bca-4488-a45a-f2a0c439704a",
   "metadata": {},
   "source": [
    "### Here use Begin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5012a5a5-f7c4-447b-9549-92def5a5f1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV data uploaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, MetaData, Table\n",
    "import numpy as np\n",
    "\n",
    "# Database connection setup\n",
    "DATABASE_URL = \"mysql+pymysql://root:@localhost/csvdata01_02102024\"\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "# Define chunk size (adjust based on available memory)\n",
    "chunk_size = 10000  # Adjust as needed based on your system's memory\n",
    "\n",
    "# Read the CSV in chunks and process each chunk\n",
    "file = 'D:/ratehawk_data_upload_database/partner_feed_en_v3_full.csv'\n",
    "\n",
    "# Use the existing 'ratehawk' table\n",
    "metadata = MetaData()\n",
    "ratehawk_table = Table('ratehawk', metadata, autoload_with=engine)\n",
    "\n",
    "def handle_nan_values(data):\n",
    "    \"\"\"\n",
    "    Replace NaN values in the dictionary with None.\n",
    "    This ensures compatibility with MySQL.\n",
    "    \"\"\"\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, float) and (pd.isna(value) or np.isnan(value)):\n",
    "            data[key] = None  # Replace NaN with None\n",
    "    return data\n",
    "\n",
    "# Process CSV in chunks\n",
    "for chunk in pd.read_csv(file, chunksize=chunk_size):\n",
    "    # Iterate over each row in the chunk\n",
    "    with engine.connect() as conn:\n",
    "        for _, row in chunk.iterrows():\n",
    "            # Convert the row to a dictionary\n",
    "            data = row.to_dict()\n",
    "\n",
    "            # Ensure that data matches the table columns\n",
    "            filtered_data = {k: v for k, v in data.items() if k in ratehawk_table.columns}\n",
    "\n",
    "            # Handle NaN values before inserting\n",
    "            filtered_data = handle_nan_values(filtered_data)\n",
    "\n",
    "            # Prepare and execute the insert statement in an individual transaction\n",
    "            with conn.begin() as transaction:\n",
    "                insert_statement = ratehawk_table.insert().values(filtered_data)\n",
    "                conn.execute(insert_statement)\n",
    "\n",
    "print(\"CSV data uploaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ef660b-0904-4324-9125-e1e2b2b8b8a8",
   "metadata": {},
   "source": [
    "### Convert zstd file to csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8352e34-0a38-4595-b193-559772c25d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting decompression...\n",
      "Opened D:/ratehawk_data_upload_database/partner_feed_en_v3.jsonl.zst for decompression.\n",
      "Opening D:/ratehawk_data_upload_database/partner_feed_en_v3.jsonl.csv for writing decompressed data.\n",
      "Decompression completed successfully.\n",
      "Decompression complete, CSV saved at: D:/ratehawk_data_upload_database/partner_feed_en_v3.jsonl.csv\n"
     ]
    }
   ],
   "source": [
    "import zstandard as zstd\n",
    "\n",
    "def extract_zst_to_csv(zst_file, csv_file):\n",
    "    try:\n",
    "        print(\"Starting decompression...\")\n",
    "        with open(zst_file, 'rb') as compressed:\n",
    "            print(f\"Opened {zst_file} for decompression.\")\n",
    "            dctx = zstd.ZstdDecompressor()\n",
    "\n",
    "            # Stream decompression\n",
    "            with open(csv_file, 'wb') as out_csv:\n",
    "                print(f\"Opening {csv_file} for writing decompressed data.\")\n",
    "                dctx.copy_stream(compressed, out_csv)\n",
    "                print(\"Decompression completed successfully.\")\n",
    "                \n",
    "        return csv_file\n",
    "\n",
    "    except zstd.ZstdError as e:\n",
    "        print(f\"Zstandard decompression error: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error during decompression: {e}\")\n",
    "        return None\n",
    "\n",
    "zst_file_path = \"D:/ratehawk_data_upload_database/partner_feed_en_v3.jsonl.zst\"\n",
    "csv_file = \"D:/ratehawk_data_upload_database/partner_feed_en_v3.jsonl.csv\"\n",
    "\n",
    "data = extract_zst_to_csv(zst_file_path, csv_file)\n",
    "\n",
    "if data:\n",
    "    print(f\"Decompression complete, CSV saved at: {data}\")\n",
    "else:\n",
    "    print(\"Decompression failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ee2364-9858-4482-a6c6-44f2b00a3ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f11449d-478a-4794-add9-579e5b3cd271",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6df1667-a058-4bce-bb69-83bf9d6e7c39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fd6e96-9da2-4627-b719-e8a81ff962c6",
   "metadata": {},
   "source": [
    "### Execution data selected rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17b30b1-ea28-4006-94b7-11610f24f392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def convert_jsonl_to_csv(jsonl_file, csv_file, num_rows=1000):\n",
    "    try:\n",
    "        rows = []\n",
    "        \n",
    "        # Open the JSONL file and read it line-by-line\n",
    "        with open(jsonl_file, 'r', encoding='ISO-8859-1') as file:\n",
    "            for i, line in enumerate(file):\n",
    "                if i >= num_rows:\n",
    "                    break  # Stop after reaching the required number of rows\n",
    "\n",
    "                try:\n",
    "                    # Parse each line as JSON\n",
    "                    row_data = json.loads(line)\n",
    "                    rows.append(row_data)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Skipping malformed line {i}: {e}\")\n",
    "\n",
    "        # Convert the list of dictionaries (rows) to a DataFrame\n",
    "        df = pd.DataFrame(rows)\n",
    "        \n",
    "        # Save the DataFrame to a CSV file\n",
    "        df.to_csv(csv_file, index=False, encoding='utf-8')\n",
    "        print(f\"Successfully converted to CSV and saved at: {csv_file}\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing JSONL to CSV: {e}\")\n",
    "        return None\n",
    "\n",
    "jsonl_file = 'D:/ratehawk_data_upload_database/partner_feed_en_v3.jsonl.csv'  \n",
    "csv_output_file = 'D:/ratehawk_data_upload_database/partner_feed_en_v3_subset.csv'  \n",
    "\n",
    "# Execute the conversion and check output\n",
    "data = convert_jsonl_to_csv(jsonl_file, csv_output_file)\n",
    "print(data if data is not None else \"Conversion failed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e486d6-ba1b-463d-ac94-754f7ad31adb",
   "metadata": {},
   "source": [
    "### Convert all data to json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb088e88-74c4-4405-8dc8-adba72bf1ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing JSONL to CSV: \n",
      "Conversion failed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def convert_jsonl_to_csv(jsonl_file, csv_file, num_rows=None):\n",
    "    try:\n",
    "        rows = []\n",
    "        processed_count = 0\n",
    "        skipped_count = 0\n",
    "        \n",
    "        # Open the JSONL file and read it line-by-line\n",
    "        with open(jsonl_file, 'r', encoding='ISO-8859-1') as file:\n",
    "            line_count = 0\n",
    "            for i, line in enumerate(file):\n",
    "                if num_rows and line_count >= num_rows:\n",
    "                    break \n",
    "                line_count += 1\n",
    "\n",
    "                try:\n",
    "                    # Parse each line as JSON\n",
    "                    row_data = json.loads(line)\n",
    "                    rows.append(row_data)\n",
    "                    processed_count += 1 \n",
    "                except json.JSONDecodeError as e:\n",
    "                    skipped_count += 1 \n",
    "                    print(f\"Skipping malformed line {i}: {e}\")\n",
    "                except Exception as e:\n",
    "                    skipped_count += 1  \n",
    "                    print(f\"Unexpected error in line {i}: {e}\")\n",
    "\n",
    "        if not rows:\n",
    "            print(\"No valid data found in the JSONL file.\")\n",
    "            return None\n",
    "\n",
    "        # Convert the list of dictionaries (rows) to a DataFrame\n",
    "        df = pd.DataFrame(rows)\n",
    "        \n",
    "        # Save the DataFrame to a CSV file\n",
    "        df.to_csv(csv_file, index=False, encoding='utf-8')\n",
    "        print(f\"Successfully converted to CSV and saved at: {csv_file}\")\n",
    "        print(f\"Processed {processed_count} rows, skipped {skipped_count} rows.\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing JSONL to CSV: {e}\")\n",
    "        return None\n",
    "\n",
    "jsonl_file = 'D:/ratehawk_data_upload_database/partner_feed_en_v3.jsonl.csv'  \n",
    "csv_output_file = 'D:/ratehawk_data_upload_database/partner_feed_en_v3_subset_data_all.csv' \n",
    "\n",
    "# Execute the conversion and check output\n",
    "data = convert_jsonl_to_csv(jsonl_file, csv_output_file)\n",
    "print(data if data is not None else \"Conversion failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9db0752-0cf5-41dd-9a94-3cc841e81dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted to CSV and saved at: D:/ratehawk_data_upload_database/partner_feed_en_v3_full.csv\n",
      "Processed 2606315 rows, skipped 0 rows.\n",
      "Conversion completed with 2606315 rows processed and 0 rows skipped.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "\n",
    "def convert_jsonl_to_csv(jsonl_file, csv_file, batch_size=100):\n",
    "    try:\n",
    "        processed_count = 0\n",
    "        skipped_count = 0\n",
    "        \n",
    "        # Open the CSV file to write\n",
    "        with open(csv_file, 'w', newline='', encoding='utf-8') as csv_output:\n",
    "            writer = None  # CSV writer will be initialized after the first batch\n",
    "            rows = []\n",
    "\n",
    "            # Open the JSONL file and read it line-by-line\n",
    "            with open(jsonl_file, 'r', encoding='ISO-8859-1') as file:\n",
    "                for i, line in enumerate(file):\n",
    "                    try:\n",
    "                        # Parse each line as JSON\n",
    "                        row_data = json.loads(line)\n",
    "                        rows.append(row_data)\n",
    "                        processed_count += 1\n",
    "\n",
    "                        # If batch is ready, write to CSV\n",
    "                        if len(rows) >= batch_size:\n",
    "                            df = pd.DataFrame(rows)\n",
    "                            if writer is None:\n",
    "                                # Initialize writer and write header for the first batch\n",
    "                                writer = csv.DictWriter(csv_output, fieldnames=df.columns)\n",
    "                                writer.writeheader()\n",
    "                            writer.writerows(df.to_dict(orient='records'))\n",
    "                            rows = []  # Clear the batch\n",
    "\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        skipped_count += 1\n",
    "                        print(f\"Skipping malformed line {i}: {e}\")\n",
    "                    except Exception as e:\n",
    "                        skipped_count += 1\n",
    "                        print(f\"Unexpected error in line {i}: {e}\")\n",
    "\n",
    "            # Write any remaining rows in the last batch\n",
    "            if rows:\n",
    "                df = pd.DataFrame(rows)\n",
    "                if writer is None:\n",
    "                    writer = csv.DictWriter(csv_output, fieldnames=df.columns)\n",
    "                    writer.writeheader()\n",
    "                writer.writerows(df.to_dict(orient='records'))\n",
    "\n",
    "        print(f\"Successfully converted to CSV and saved at: {csv_file}\")\n",
    "        print(f\"Processed {processed_count} rows, skipped {skipped_count} rows.\")\n",
    "        return processed_count, skipped_count\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing JSONL to CSV: {e}\")\n",
    "        return None\n",
    "\n",
    "# File paths\n",
    "jsonl_file = 'D:/ratehawk_data_upload_database/partner_feed_en_v3.jsonl.csv'  \n",
    "csv_output_file = 'D:/ratehawk_data_upload_database/partner_feed_en_v3_full.csv'  \n",
    "\n",
    "# Execute the conversion and check output\n",
    "processed, skipped = convert_jsonl_to_csv(jsonl_file, csv_output_file)\n",
    "if processed is not None:\n",
    "    print(f\"Conversion completed with {processed} rows processed and {skipped} rows skipped.\")\n",
    "else:\n",
    "    print(\"Conversion failed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc35a86-0ddc-4bcd-bf62-150e0f02c662",
   "metadata": {},
   "source": [
    "## Get only header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d85ee4d-68b9-4e44-a5c8-6f7342143350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [address, amenity_groups, check_in_time, check_out_time, description_struct, id, hid, images, images_ext, kind, latitude, longitude, name, phone, policy_struct, postal_code, room_groups, region, star_rating, email, serp_filters, deleted, is_closed, is_gender_specification_required, metapolicy_struct, metapolicy_extra_info, star_certificate, facts, payment_methods, hotel_chain, front_desk_time_start, front_desk_time_end, keys_pickup]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def convert_jsonl_to_csv_with_header(jsonl_file, csv_file, num_rows=2):\n",
    "    try:\n",
    "        rows = []\n",
    "        \n",
    "        # Open the JSONL file and read it line-by-line\n",
    "        with open(jsonl_file, 'r', encoding='ISO-8859-1') as file:\n",
    "            for i, line in enumerate(file):\n",
    "                if i >= num_rows:\n",
    "                    break \n",
    "\n",
    "                try:\n",
    "                    # Parse each line as JSON\n",
    "                    row_data = json.loads(line)\n",
    "                    rows.append(row_data)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Skipping malformed line {i}: {e}\")\n",
    "\n",
    "        # Convert the list of dictionaries (rows) to a DataFrame\n",
    "        df = pd.DataFrame(rows)\n",
    "        \n",
    "        # Extract headers (column names)\n",
    "        headers = df.columns.tolist()\n",
    "\n",
    "        # Create an empty DataFrame with only the headers\n",
    "        df_header = pd.DataFrame(columns=headers)\n",
    "        \n",
    "        # Save the DataFrame with only headers to a CSV file\n",
    "        df_header.to_csv(csv_file, index=False, encoding='utf-8')\n",
    "        # print(f\"Successfully created header-only CSV and saved at: {csv_file}\")\n",
    "        return df_header\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing JSONL to header-only CSV: {e}\")\n",
    "        return None\n",
    "\n",
    "jsonl_file = 'D:/ratehawk_data_upload_database/partner_feed_en_v3.jsonl.csv' \n",
    "csv_output_file = 'D:/ratehawk_data_upload_database/partner_feed_en_v3_header.csv'  \n",
    "\n",
    "# Execute the conversion and check output\n",
    "header_data = convert_jsonl_to_csv_with_header(jsonl_file, csv_output_file)\n",
    "print(header_data if header_data is not None else \"Header extraction failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eac8f9-2418-4613-9087-78b4bb2f0189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2be9feba-6e06-4e53-bd5c-7a2ccdaf2c59",
   "metadata": {},
   "source": [
    "### Get data from localdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13ad7c96-685a-42d8-aa60-ba06b3efdd94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, MetaData, Table\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Database connection setup\n",
    "DATABASE_URL = \"mysql+pymysql://root:@localhost/csvdata01_02102024\"\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "table = \"ratehawk\"\n",
    "\n",
    "query = f\"SELECT * FROM {table} LIMIT 1 OFFSET 4\"\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    first_row = pd.read_sql_query(query, conn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b34893-6948-44c1-9cfc-aa7ed73dadf5",
   "metadata": {},
   "source": [
    "### Convert table row into the json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dbb38c-f69e-4fa5-a332-f7ba3f497f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row_dict = first_row.astype(str).to_dict(orient=\"records\")[0] \n",
    "# first_row_dict = first_row.to_dict(orient=\"records\")[0] \n",
    "first_row_json = json.dumps(first_row_dict, indent=4) \n",
    "\n",
    "print(first_row_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6946c122-8696-44ec-a0f7-4a2f3336499d",
   "metadata": {},
   "source": [
    "### Get a selectted row data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc23f6f1-c945-4fca-8c50-6d5e20431db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT amenity_groups FROM ratehawk WHERE my_id = '3199216';\"\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    column = pd.read_sql_query(query, conn)\n",
    "    data = column.iloc[0,0]\n",
    "    print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb6b764-ce00-4688-811b-f62e89709bdc",
   "metadata": {},
   "source": [
    "### Create json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9b3b174d-f6ba-4fff-b74f-b06028d8fbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"amenity_groups\": \"[{'amenities': ['Shopping on site', 'Air conditioning', 'Currency exchange', 'Smoke-free property', 'Security guard', 'Ticket assistance', 'Gift shop', 'Express check-in/check-out', 'Garden', 'Television in lobby', 'Terrace', 'Reception desk'], 'non_free_amenities': [], 'group_name': 'General'}, {'amenities': ['Bridal suite', 'Non-smoking rooms', 'Room service', 'Family room', 'Minibar', 'Toiletries'], 'non_free_amenities': [], 'group_name': 'Rooms'}, {'amenities': ['Accessibility features'], 'non_free_amenities': [], 'group_name': 'Accessibility'}, {'amenities': ['Ironing', 'Luggage storage', 'Laundry', 'Safe-deposit box', 'Concierge services', 'Dry-cleaning', 'Luggage storage', 'Telephone'], 'non_free_amenities': ['Ironing', 'Laundry', 'Dry-cleaning'], 'group_name': 'Services and amenities'}, {'amenities': ['Bar', 'Coffee/tea for guests', 'Breakfast', 'Buffet breakfast', 'Breakfast in the room', 'Cafe', 'Restaurant', 'Kitchen', 'Bottled water', 'Kettle'], 'non_free_amenities': [], 'group_name': 'Meals'}, {'amenities': ['Free Wi-Fi', 'Internet'], 'non_free_amenities': ['Internet'], 'group_name': 'Internet'}, {'amenities': ['Shuttle', 'Airport transportation', 'Transfer services'], 'non_free_amenities': ['Shuttle'], 'group_name': 'Transfer'}, {'amenities': ['English', 'French'], 'non_free_amenities': [], 'group_name': 'Languages Spoken'}, {'amenities': ['Tour assistance'], 'non_free_amenities': [], 'group_name': 'Tourist services'}, {'amenities': ['Free bicycle rental', 'Library', 'Bike rental', 'Barbecue grill(s)', 'Hiking', 'Picnic area', 'Fishing', 'Barbeque', 'Sun Deck'], 'non_free_amenities': [], 'group_name': 'Recreation'}, {'amenities': ['Free parking'], 'non_free_amenities': [], 'group_name': 'Parking'}, {'amenities': ['Swimming pool', 'Outdoor pool', 'Next to the beach'], 'non_free_amenities': [], 'group_name': 'Pool and beach'}, {'amenities': ['Event facilities', 'Fax and copy machine', 'Conference Hall'], 'non_free_amenities': [], 'group_name': 'Business'}, {'amenities': ['Diving', 'Cycling', 'Snorkelling', 'Fitness facilities', 'Gym'], 'non_free_amenities': [], 'group_name': 'Sports'}, {'amenities': ['Doctor', 'Massage', 'Spa', 'First Aid Kit'], 'non_free_amenities': ['Massage', 'Spa'], 'group_name': 'Beauty and wellness'}, {'amenities': ['Babysitting and childcare', 'Family/Kid Friendly'], 'non_free_amenities': [], 'group_name': 'Kids'}, {'amenities': ['Temperature control for staff', 'Extra decontamination measures', 'Temperature control for guests', 'Additional measures against COVID-19'], 'non_free_amenities': [], 'group_name': 'Health and Safety Measures'}]\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Database connection setup\n",
    "DATABASE_URL = \"mysql+pymysql://root:@localhost/csvdata01_02102024\"\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "# Define query\n",
    "query = \"SELECT amenity_groups FROM ratehawk WHERE my_id = '3199216';\"\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    column = pd.read_sql_query(query, conn)\n",
    "    \n",
    "    # Retrieve the data if available\n",
    "    if not column.empty:\n",
    "        data = column.iloc[0, 0]  # First row, first column\n",
    "        data_dict = {\"amenity_groups\": data}  # Prepare data as a dictionary for JSON\n",
    "        \n",
    "        # Convert the dictionary to a JSON-formatted string and print\n",
    "        json_output = json.dumps(data_dict, indent=4)\n",
    "        print(json_output)\n",
    "    else:\n",
    "        print(\"No matching data found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8f3aef62-0e9d-4ac1-96d8-3117489e8f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'amenity_groups': \"[{'amenities': ['Shopping on site', 'Air conditioning', 'Currency exchange', 'Smoke-free property', 'Security guard', 'Ticket assistance', 'Gift shop', 'Express check-in/check-out', 'Garden', 'Television in lobby', 'Terrace', 'Reception desk'], 'non_free_amenities': [], 'group_name': 'General'}, {'amenities': ['Bridal suite', 'Non-smoking rooms', 'Room service', 'Family room', 'Minibar', 'Toiletries'], 'non_free_amenities': [], 'group_name': 'Rooms'}, {'amenities': ['Accessibility features'], 'non_free_amenities': [], 'group_name': 'Accessibility'}, {'amenities': ['Ironing', 'Luggage storage', 'Laundry', 'Safe-deposit box', 'Concierge services', 'Dry-cleaning', 'Luggage storage', 'Telephone'], 'non_free_amenities': ['Ironing', 'Laundry', 'Dry-cleaning'], 'group_name': 'Services and amenities'}, {'amenities': ['Bar', 'Coffee/tea for guests', 'Breakfast', 'Buffet breakfast', 'Breakfast in the room', 'Cafe', 'Restaurant', 'Kitchen', 'Bottled water', 'Kettle'], 'non_free_amenities': [], 'group_name': 'Meals'}, {'amenities': ['Free Wi-Fi', 'Internet'], 'non_free_amenities': ['Internet'], 'group_name': 'Internet'}, {'amenities': ['Shuttle', 'Airport transportation', 'Transfer services'], 'non_free_amenities': ['Shuttle'], 'group_name': 'Transfer'}, {'amenities': ['English', 'French'], 'non_free_amenities': [], 'group_name': 'Languages Spoken'}, {'amenities': ['Tour assistance'], 'non_free_amenities': [], 'group_name': 'Tourist services'}, {'amenities': ['Free bicycle rental', 'Library', 'Bike rental', 'Barbecue grill(s)', 'Hiking', 'Picnic area', 'Fishing', 'Barbeque', 'Sun Deck'], 'non_free_amenities': [], 'group_name': 'Recreation'}, {'amenities': ['Free parking'], 'non_free_amenities': [], 'group_name': 'Parking'}, {'amenities': ['Swimming pool', 'Outdoor pool', 'Next to the beach'], 'non_free_amenities': [], 'group_name': 'Pool and beach'}, {'amenities': ['Event facilities', 'Fax and copy machine', 'Conference Hall'], 'non_free_amenities': [], 'group_name': 'Business'}, {'amenities': ['Diving', 'Cycling', 'Snorkelling', 'Fitness facilities', 'Gym'], 'non_free_amenities': [], 'group_name': 'Sports'}, {'amenities': ['Doctor', 'Massage', 'Spa', 'First Aid Kit'], 'non_free_amenities': ['Massage', 'Spa'], 'group_name': 'Beauty and wellness'}, {'amenities': ['Babysitting and childcare', 'Family/Kid Friendly'], 'non_free_amenities': [], 'group_name': 'Kids'}, {'amenities': ['Temperature control for staff', 'Extra decontamination measures', 'Temperature control for guests', 'Additional measures against COVID-19'], 'non_free_amenities': [], 'group_name': 'Health and Safety Measures'}]\"}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Database connection setup\n",
    "DATABASE_URL = \"mysql+pymysql://root:@localhost/csvdata01_02102024\"\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "# Define query\n",
    "query = \"SELECT amenity_groups FROM ratehawk WHERE my_id = '3199216';\"\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    column = pd.read_sql_query(query, conn)\n",
    "    \n",
    "    # Retrieve the data if available\n",
    "    if not column.empty:\n",
    "        data = column.iloc[0, 0]  # First row, first column\n",
    "        data_dict = {\"amenity_groups\": data}  # Store data in a dictionary\n",
    "        \n",
    "        # Print the dictionary\n",
    "        print(data_dict)\n",
    "    else:\n",
    "        print(\"No matching data found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c4550dc8-c3f1-41a8-b12c-c40e7fb2a590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"[{'amenities': ['Shopping on site', 'Air conditioning', 'Currency exchange', 'Smoke-free property', 'Security guard', 'Ticket assistance', 'Gift shop', 'Express check-in/check-out', 'Garden', 'Television in lobby', 'Terrace', 'Reception desk'], 'non_free_amenities': [], 'group_name': 'General'}, {'amenities': ['Bridal suite', 'Non-smoking rooms', 'Room service', 'Family room', 'Minibar', 'Toiletries'], 'non_free_amenities': [], 'group_name': 'Rooms'}, {'amenities': ['Accessibility features'], 'non_free_amenities': [], 'group_name': 'Accessibility'}, {'amenities': ['Ironing', 'Luggage storage', 'Laundry', 'Safe-deposit box', 'Concierge services', 'Dry-cleaning', 'Luggage storage', 'Telephone'], 'non_free_amenities': ['Ironing', 'Laundry', 'Dry-cleaning'], 'group_name': 'Services and amenities'}, {'amenities': ['Bar', 'Coffee/tea for guests', 'Breakfast', 'Buffet breakfast', 'Breakfast in the room', 'Cafe', 'Restaurant', 'Kitchen', 'Bottled water', 'Kettle'], 'non_free_amenities': [], 'group_name': 'Meals'}, {'amenities': ['Free Wi-Fi', 'Internet'], 'non_free_amenities': ['Internet'], 'group_name': 'Internet'}, {'amenities': ['Shuttle', 'Airport transportation', 'Transfer services'], 'non_free_amenities': ['Shuttle'], 'group_name': 'Transfer'}, {'amenities': ['English', 'French'], 'non_free_amenities': [], 'group_name': 'Languages Spoken'}, {'amenities': ['Tour assistance'], 'non_free_amenities': [], 'group_name': 'Tourist services'}, {'amenities': ['Free bicycle rental', 'Library', 'Bike rental', 'Barbecue grill(s)', 'Hiking', 'Picnic area', 'Fishing', 'Barbeque', 'Sun Deck'], 'non_free_amenities': [], 'group_name': 'Recreation'}, {'amenities': ['Free parking'], 'non_free_amenities': [], 'group_name': 'Parking'}, {'amenities': ['Swimming pool', 'Outdoor pool', 'Next to the beach'], 'non_free_amenities': [], 'group_name': 'Pool and beach'}, {'amenities': ['Event facilities', 'Fax and copy machine', 'Conference Hall'], 'non_free_amenities': [], 'group_name': 'Business'}, {'amenities': ['Diving', 'Cycling', 'Snorkelling', 'Fitness facilities', 'Gym'], 'non_free_amenities': [], 'group_name': 'Sports'}, {'amenities': ['Doctor', 'Massage', 'Spa', 'First Aid Kit'], 'non_free_amenities': ['Massage', 'Spa'], 'group_name': 'Beauty and wellness'}, {'amenities': ['Babysitting and childcare', 'Family/Kid Friendly'], 'non_free_amenities': [], 'group_name': 'Kids'}, {'amenities': ['Temperature control for staff', 'Extra decontamination measures', 'Temperature control for guests', 'Additional measures against COVID-19'], 'non_free_amenities': [], 'group_name': 'Health and Safety Measures'}]\"\n"
     ]
    }
   ],
   "source": [
    "json_data_in_column_index = json.dumps(data, indent=4)\n",
    "print(json_data_in_column_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d52dcef6-4eeb-4c18-ad5e-ccd405ccf348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      amenity_groups\n",
      "0  [{'amenities': ['Shopping on site', 'Air condi...\n"
     ]
    }
   ],
   "source": [
    "query = \"SELECT amenity_groups FROM ratehawk WHERE my_id = 3199216;\"\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    column = pd.read_sql_query(query, conn)\n",
    "    print(column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3a364c-af9d-4633-95c7-07d440500815",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93985e8e-3360-433e-a538-b2b4ac7818b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql(engine, table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
